# 采集策略重大更新 - V4.2

## 问题

V4.1 的问题：
- **批量收集链接 → 逐个打开** 的行为模式
- 会触发反爬机制，看不到笔记详情
- 不是真实的人工行为

## 新策略 (V4.2)

### 模拟真实人工行为
```
搜索页面 
  ↓ 滚动
找到笔记 → 点击 → 查看详情 → 返回搜索页
  ↓ 继续滚动
下一个笔记 → 点击 → 查看详情 → 返回搜索页
  ↓ ...
直到收集够数量
```

### 关键改变

#### 旧流程 (V4.1)
```python
1. 打开搜索页面
2. 滚动收集所有笔记链接 (一次性收集100个)
3. 逐个打开链接收集详情
4. 问题：批量访问会触发反爬
```

#### 新流程 (V4.2)
```python
1. 打开搜索页面
2. 滚动找到笔记元素
3. 点击第一个笔记 (人工模拟)
4. 收集详情
5. 返回搜索页 (page.go_back())
6. 继续滚动找下一个笔记
7. 重复直到收集够数量
8. 优点：完全模拟人工浏览行为
```

### 核心函数

#### 1. `open_search_page()` - 打开搜索页
- 尝试多个搜索URL
- 检测登录弹窗
- 模拟人工滚动

#### 2. `get_note_elements()` - 获取笔记元素
- 使用多个选择器
- 找到页面上的所有笔记

#### 3. `scroll_search_page()` - 滚动页面
- 模拟人工随机滚动
- 鼠标移动
- 随机延迟

#### 4. `crawl_keyword()` - 主采集流程
```
循环直到收集够数量：
  1. 获取笔记元素
  2. 找第一个未访问的笔记
  3. 点击 (人工模拟)
  4. 等待加载
  5. 提取详情
  6. 返回搜索页
  7. 滚动到下一页
  8. 随机延迟
```

### 人工点击模拟

```python
# 鼠标移动到元素
target_x = box['x'] + box['width'] / 2 + offset_x
target_y = box['y'] + box['height'] / 2 + offset_y
page.mouse.move(target_x, target_y, steps=random.randint(5, 10))

# 点击
elem.click()

# 随机延迟
time.sleep(random.uniform(0.3, 0.8))
```

### 配置项

```env
# 关键配置
ENABLE_CLICK_SIMULATION=true  # 启用点击模拟
SLEEP_BETWEEN_REQUESTS=3.5    # 请求间隔
SLEEP_BETWEEN_SCROLLS=1.5     # 滚动间隔
TARGET_COUNT=100              # 目标数量
```

### 优势

1. **完全模拟人工**：浏览 → 点击 → 返回 → 继续
2. **反爬友好**：不会批量访问同一类型页面
3. **真实间隔**：每次点击都有随机延迟
4. **可恢复**：支持断点续爬
5. **调试友好**：保存截图和HTML

### 运行测试

```bash
cd D:\xiaohongshu-crawler
python crawler_v4.py
```

### 预期效果

- ✅ 搜索页面正常加载
- ✅ 逐个点击笔记（像人工一样）
- ✅ 笔记详情正常显示
- ✅ 不会触发反爬
- ✅ 成功收集100条

### 如果失败

1. 检查 `output/debug/` 目录的截图
2. 删除 `cookies.json` 重新登录
3. 增加 `SLEEP_BETWEEN_REQUESTS` 的值
4. 检查网络连接

### 与V4.1的区别

| 特性 | V4.1 | V4.2 |
|------|------|------|
| 链接收集 | 批量收集100个 | 实时获取 |
| 访问方式 | 逐个打开链接 | 点击→返回 |
| 反爬风险 | 高（批量访问） | 低（模拟人工） |
| 行为模式 | 机器人 | 人类 |
| 页面停留 | 不确定 | 随机 |

### 总结

V4.2 采用**完全模拟人工浏览**的策略，而不是批量收集链接。这样可以有效避免反爬机制，稳定收集笔记详情。
