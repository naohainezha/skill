"""
å°çº¢ä¹¦çˆ¬è™«æ”¹è¿›ç‰ˆ - æ”¯æŒåŠ¨æ€é¡µé¢è§£æå’Œé”™è¯¯æ¢å¤
"""
import asyncio
import json
import time
import random
import os
import sys
import io
from datetime import datetime
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from config import Config

# è®¾ç½®UTF-8ç¼–ç ï¼ˆWindowså…¼å®¹ï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')


class XiaohongshuCrawlerV2:
    def __init__(self):
        self.notes = []
        self.browser = None
        self.context = None
        self.page = None
        self.note_urls = set()
        
    async def init(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(
            headless=Config.HEADLESS,
            args=Config.BROWSER_ARGS
        )
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¿å­˜çš„cookies
        cookies = []
        if os.path.exists(Config.COOKIE_FILE):
            try:
                with open(Config.COOKIE_FILE, 'r', encoding='utf-8') as f:
                    cookies = json.load(f)
                print(f"âœ“ åŠ è½½äº† {len(cookies)} ä¸ªcookies")
            except Exception as e:
                print(f"âš  åŠ è½½cookieså¤±è´¥: {e}")
        
        # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
        self.context = await self.browser.new_context(
            user_agent=Config.USER_AGENT,
            viewport={'width': 1920, 'height': 1080},
            storage_state={'cookies': cookies},
            locale='zh-CN',
            timezone_id='Asia/Shanghai'
        )
        
        self.page = await self.context.new_page()
        
        # æ³¨å…¥åæ£€æµ‹è„šæœ¬
        await self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5]
            });
            
            Object.defineProperty(navigator, 'languages', {
                get: () => ['zh-CN', 'zh', 'en']
            });
            
            window.chrome = {
                runtime: {}
            };
            
            Object.defineProperty(navigator, 'permissions', {
                get: () => ({
                    query: () => Promise.resolve({ state: 'granted' })
                })
            });
        """)
        
        # æ·»åŠ ç½‘ç»œè¯·æ±‚ç›‘å¬ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
        # await self.page.on('request', lambda request: print(f"è¯·æ±‚: {request.url}"))
    
    async def save_cookies(self):
        """ä¿å­˜cookies"""
        cookies = await self.context.cookies()
        with open(Config.COOKIE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cookies, f, ensure_ascii=False, indent=2)
        print(f"âœ“ Cookieså·²ä¿å­˜åˆ° {Config.COOKIE_FILE}")
    
    async def check_login_status(self):
        """æ£€æŸ¥æ˜¯å¦å·²ç™»å½•"""
        await self.page.goto("https://www.xiaohongshu.com/")
        await asyncio.sleep(3)
        
        try:
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰ç™»å½•æŒ‰é’®æˆ–ç”¨æˆ·å¤´åƒ
            avatar = await self.page.query_selector('[class*="avatar"], [class*="user"]')
            if avatar:
                print("âœ“ æ£€æµ‹åˆ°ç™»å½•çŠ¶æ€")
                return True
            
            login_btn = await self.page.query_selector('text=ç™»å½•')
            if login_btn:
                print("âš  æœªç™»å½•")
                return False
            
            # æ£€æŸ¥URLæ˜¯å¦è·³è½¬åˆ°ç™»å½•é¡µ
            current_url = self.page.url
            if 'login' in current_url.lower():
                print("âš  æœªç™»å½•")
                return False
                
            print("âœ“ æ£€æµ‹åˆ°ç™»å½•çŠ¶æ€")
            return True
            
        except Exception as e:
            print(f"âš  æ£€æŸ¥ç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {e}")
            return False
    
    async def login_with_qrcode(self):
        """ä½¿ç”¨äºŒç»´ç ç™»å½•"""
        print("\n=== å¼€å§‹ç™»å½•æµç¨‹ ===")
        await self.page.goto("https://www.xiaohongshu.com/")
        await asyncio.sleep(3)
        
        try:
            # æŸ¥æ‰¾ç™»å½•æŒ‰é’®ï¼ˆå¤šç§å¯èƒ½çš„é€‰æ‹©å™¨ï¼‰
            login_selectors = [
                'text=ç™»å½•',
                '[class*="login-btn"]',
                'button:has-text("ç™»å½•")',
                'a:has-text("ç™»å½•")'
            ]
            
            login_btn = None
            for selector in login_selectors:
                try:
                    login_btn = await self.page.wait_for_selector(selector, timeout=5000)
                    if login_btn:
                        break
                except:
                    continue
            
            if login_btn:
                await login_btn.click()
                await asyncio.sleep(3)
                
                # æŸ¥æ‰¾äºŒç»´ç ç™»å½•é€‰é¡¹
                qrcode_selectors = [
                    'text=æ‰«ç ç™»å½•',
                    '[class*="qrcode"]',
                    'text=å…¶ä»–æ–¹å¼ç™»å½•'
                ]
                
                qrcode_clicked = False
                for selector in qrcode_selectors:
                    try:
                        qrcode_elem = await self.page.query_selector(selector)
                        if qrcode_elem:
                            await qrcode_elem.click()
                            qrcode_clicked = True
                            break
                    except:
                        continue
                
                print("ğŸ“± è¯·ä½¿ç”¨å°çº¢ä¹¦APPæ‰«æäºŒç»´ç ç™»å½•...")
                print("â³ ç­‰å¾…ç™»å½•ï¼ˆæœ€å¤šç­‰å¾…2åˆ†é’Ÿï¼‰...")
                
                # ç­‰å¾…ç™»å½•æˆåŠŸ
                try:
                    await self.page.wait_for_url("**/home**", timeout=120000)
                    print("âœ“ ç™»å½•æˆåŠŸï¼")
                    await self.save_cookies()
                    return True
                except:
                    print("âš  ç­‰å¾…ç™»å½•è¶…æ—¶")
                    
        except Exception as e:
            print(f"âŒ ç™»å½•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        
        return False
    
    async def search_notes(self, keyword):
        """æœç´¢ç¬”è®°"""
        # æ–¹æ¡ˆ1: ç›´æ¥æœç´¢URL
        search_url = f"https://www.xiaohongshu.com/search_result?keyword={keyword}"
        print(f"\nğŸ” æ­£åœ¨æœç´¢: {keyword}")
        await self.page.goto(search_url)
        await asyncio.sleep(random.uniform(3, 5))
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†ç™»å½•æç¤º
        try:
            await self.page.wait_for_selector('body', timeout=10000)
        except:
            pass
    
    async def extract_note_links_from_list_page(self):
        """ä»åˆ—è¡¨é¡µæå–ç¬”è®°é“¾æ¥"""
        note_links = []
        
        try:
            # å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨æ¥æŸ¥æ‰¾ç¬”è®°é“¾æ¥
            selectors = [
                'a[href*="/explore/"]',
                'article a',
                '[class*="note-item"] a',
                '[class*="feed-card"] a'
            ]
            
            for selector in selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    for elem in elements:
                        try:
                            href = await elem.get_attribute('href')
                            if href and '/explore/' in href:
                                full_url = href if href.startswith('http') else f"https://www.xiaohongshu.com{href}"
                                if full_url not in self.note_urls:
                                    note_links.append(full_url)
                                    self.note_urls.add(full_url)
                        except:
                            continue
                    
                    if note_links:
                        break
                        
                except:
                    continue
            
            print(f"âœ“ ä»åˆ—è¡¨é¡µæå–åˆ° {len(note_links)} ä¸ªæ–°é“¾æ¥")
            
        except Exception as e:
            print(f"âš  æå–é“¾æ¥æ—¶å‡ºé”™: {e}")
        
        return note_links
    
    async def extract_note_detail(self, url):
        """æå–ç¬”è®°è¯¦æƒ…"""
        note_data = None
        
        try:
            await self.page.goto(url, wait_until='networkidle')
            await asyncio.sleep(random.uniform(2, 3))
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            try:
                await self.page.wait_for_selector('body', timeout=10000)
            except:
                pass
            
            # æå–æ ‡é¢˜ - å¤šç§é€‰æ‹©å™¨
            title = ""
            title_selectors = [
                'h1',
                '[class*="title"]',
                '[class*="note-title"]',
                '[class*="post-title"]'
            ]
            
            for selector in title_selectors:
                try:
                    elem = await self.page.query_selector(selector)
                    if elem:
                        title = await elem.text_content()
                        if title and len(title.strip()) > 0:
                            title = title.strip()
                            break
                except:
                    continue
            
            # æå–æ­£æ–‡ - å¤šç§é€‰æ‹©å™¨
            content = ""
            content_selectors = [
                '[class*="content"]',
                '[class*="note-content"]',
                '[class*="desc"]',
                '[class*="text-content"]',
                '[class*="post-content"]',
                'article p',
                '[class*="rich-text"]'
            ]
            
            for selector in content_selectors:
                try:
                    elem = await self.page.query_selector(selector)
                    if elem:
                        content = await elem.text_content()
                        if content and len(content.strip()) > 10:
                            content = content.strip()
                            break
                except:
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ­£æ–‡ï¼Œå°è¯•è·å–æ•´ä¸ªbodyæ–‡æœ¬
            if not content or len(content) < 10:
                try:
                    body_text = await self.page.evaluate('() => document.body.innerText')
                    content = body_text.strip()
                except:
                    pass
            
            if title and (content or len(content) >= 10):
                note_data = {
                    'title': title,
                    'content': content,
                    'url': url,
                    'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                print(f"âœ“ æˆåŠŸæå–: {title[:40]}...")
            
        except Exception as e:
            print(f"âš  æå–ç¬”è®°è¯¦æƒ…æ—¶å‡ºé”™ {url}: {e}")
        
        return note_data
    
    async def scroll_and_collect_links(self):
        """æ»šåŠ¨é¡µé¢å¹¶æ”¶é›†ç¬”è®°é“¾æ¥"""
        note_links = []
        max_scrolls = 50  # æœ€å¤šæ»šåŠ¨50æ¬¡
        scroll_count = 0
        
        print("ğŸ“œ å¼€å§‹æ»šåŠ¨æ”¶é›†ç¬”è®°é“¾æ¥...")
        
        while scroll_count < max_scrolls and len(note_links) < Config.TARGET_COUNT * 2:
            # æ¯æ¬¡æ»šåŠ¨æ”¶é›†ä¸€æ¬¡é“¾æ¥
            new_links = await self.extract_note_links_from_list_page()
            note_links.extend(new_links)
            
            print(f"ğŸ“Š å½“å‰å·²æ”¶é›† {len(note_links)} ä¸ªé“¾æ¥")
            
            # æ»šåŠ¨åˆ°åº•éƒ¨
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(Config.SCROLL_DELAY)
            
            scroll_count += 1
            
            # éšæœºåœé¡¿ï¼Œé¿å…è¢«æ£€æµ‹
            if scroll_count % 5 == 0:
                await asyncio.sleep(random.uniform(1, 2))
        
        return list(set(note_links))  # å»é‡
    
    async def crawl_notes(self):
        """çˆ¬å–ç¬”è®°è¯¦æƒ…"""
        # å…ˆæ”¶é›†é“¾æ¥
        note_links = await self.scroll_and_collect_links()
        
        if not note_links:
            print("âŒ æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•ç¬”è®°é“¾æ¥")
            return
        
        print(f"\nğŸ“ å¼€å§‹æå– {min(len(note_links), Config.TARGET_COUNT)} ç¯‡ç¬”è®°è¯¦æƒ…...")
        
        # æå–ç¬”è®°è¯¦æƒ…
        for i, url in enumerate(note_links[:Config.TARGET_COUNT]):
            if len(self.notes) >= Config.TARGET_COUNT:
                break
            
            note_data = await self.extract_note_detail(url)
            if note_data:
                # æ£€æŸ¥æ˜¯å¦é‡å¤
                is_duplicate = any(n['title'] == note_data['title'] for n in self.notes)
                if not is_duplicate:
                    self.notes.append(note_data)
                    print(f"ğŸ“Œ [{len(self.notes)}/{Config.TARGET_COUNT}] {note_data['title'][:50]}...")
            
            # è¯·æ±‚é—´éš”
            await asyncio.sleep(random.uniform(1, 2))
    
    async def save_notes(self):
        """ä¿å­˜ç¬”è®°åˆ°æ–‡ä»¶"""
        if not self.notes:
            print("âš  æ²¡æœ‰ç¬”è®°éœ€è¦ä¿å­˜")
            return
        
        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"notes_{timestamp}.json"
        filepath = os.path.join(Config.OUTPUT_DIR, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.notes, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ“ æˆåŠŸä¿å­˜ {len(self.notes)} ç¯‡ç¬”è®°åˆ° {filepath}")
        
        # åŒæ—¶ä¿å­˜ä¸ºCSVæ ¼å¼
        csv_filename = f"notes_{timestamp}.csv"
        csv_filepath = os.path.join(Config.OUTPUT_DIR, csv_filename)
        
        import csv
        try:
            with open(csv_filepath, 'w', encoding='utf-8-sig', newline='') as f:
                if self.notes:
                    writer = csv.DictWriter(f, fieldnames=self.notes[0].keys())
                    writer.writeheader()
                    writer.writerows(self.notes)
            print(f"âœ“ æˆåŠŸä¿å­˜ {len(self.notes)} ç¯‡ç¬”è®°åˆ° {csv_filepath}")
        except Exception as e:
            print(f"âš  ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    async def run(self):
        """è¿è¡Œçˆ¬è™«"""
        print("=" * 60)
        print("  å°çº¢ä¹¦çˆ¬è™« V2.0 - çœ¼é•œç›¸å…³ç¬”è®°é‡‡é›†")
        print("=" * 60)
        
        await self.init()
        
        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        is_logged_in = await self.check_login_status()
        
        if not is_logged_in:
            print("æœªæ£€æµ‹åˆ°ç™»å½•çŠ¶æ€ï¼Œéœ€è¦ç™»å½•")
            login_success = await self.login_with_qrcode()
            if not login_success:
                print("âŒ ç™»å½•å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
                await self.browser.close()
                return
        else:
            print("âœ“ å·²æ£€æµ‹åˆ°ç™»å½•çŠ¶æ€")
        
        # æœç´¢ç¬”è®°
        await self.search_notes(Config.SEARCH_KEYWORD)
        
        # çˆ¬å–ç¬”è®°
        await self.crawl_notes()
        
        # ä¿å­˜ç»“æœ
        await self.save_notes()
        
        # æ›´æ–°cookies
        await self.save_cookies()
        
        # å…³é—­æµè§ˆå™¨
        await self.browser.close()
        
        print("\n" + "=" * 60)
        print(f"âœ… çˆ¬å–å®Œæˆï¼å…±é‡‡é›† {len(self.notes)} ç¯‡ç¬”è®°")
        print("=" * 60)


async def main():
    """ä¸»å‡½æ•°"""
    crawler = XiaohongshuCrawlerV2()
    await crawler.run()


if __name__ == "__main__":
    asyncio.run(main())
