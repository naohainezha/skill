# V4.2 快速启动指南

## 核心改变

✅ **V4.2**: 模拟人工浏览（点击→返回→继续）  
❌ **V4.1**: 批量收集链接→逐个打开

## 快速开始

```bash
cd D:\xiaohongshu-crawler
python crawler_v4.py
```

## 工作流程

```
1. 登录（首次需要扫码）
2. 打开搜索页面
3. 滚动找到笔记
4. 点击笔记（人工模拟）
5. 收集详情
6. 返回搜索页
7. 重复直到100条
```

## 配置说明

编辑 `.env` 文件：

```env
KEYWORDS=眼镜
TARGET_COUNT=100
ENABLE_CLICK_SIMULATION=true
SLEEP_BETWEEN_REQUESTS=3.5
```

## 预期结果

- ✅ 搜索页面正常
- ✅ 逐个点击笔记
- ✅ 笔记详情显示
- ✅ 不触发反爬
- ✅ 100条成功收集

## 输出文件

```
output/
├── notes_20260105_143022.json
├── notes_20260105_143022.csv
└── debug/
    ├── timeout_*.png
    └── timeout_*.html
```

## 问题排查

### 问题1：搜索超时
- 删除 `cookies.json`
- 重新运行扫码登录

### 问题2：看不到笔记详情
- 检查 `output/debug/` 截图
- 增加延迟时间

### 问题3：检测到验证码
- 手动完成验证
- 脚本会自动等待

## 与V4.1对比

| 项目 | V4.1 | V4.2 |
|------|------|------|
| 策略 | 批量链接 | 逐个点击 |
| 行为 | 机器人 | 人类 |
| 反爬风险 | 高 | 低 |
| 成功率 | 低 | 高 |

## 注意事项

1. **不要关闭浏览器**：采集过程中浏览器会保持打开
2. **等待登录**：首次运行需要扫码登录（最多10分钟）
3. **网络稳定**：保持网络连接稳定
4. **耐心等待**：采集100条大约需要20-30分钟

## 下次运行

- 自动加载Cookie，无需重新登录
- 如果Cookie失效，会自动提示重新登录

## 完成！

看到这个提示表示成功：
```
✅ 任务完成！
JSON文件路径: output/notes_*.json
CSV文件路径: output/notes_*.csv
```
