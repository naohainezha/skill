# 小红书爬虫项目总结

## 项目概述

这是一个使用Python和Playwright开发的小红书爬虫，可以自动搜索并采集笔记的标题和正文内容，支持扫码登录和cookies持久化。

## 核心功能

### 1. 自动化浏览
- 使用Playwright控制真实浏览器
- 模拟人工操作（点击、滚动等）
- 支持无头模式（后台运行）

### 2. 登录管理
- 支持扫码登录
- 自动保存和加载cookies
- cookies持久化，避免重复登录

### 3. 数据采集
- 搜索关键词
- 自动滚动加载更多内容
- 提取笔记标题和正文
- 支持链接去重和数据验证

### 4. 数据导出
- JSON格式（适合程序处理）
- CSV格式（适合Excel打开）
- 自动创建时间戳文件名

### 5. 反爬虫措施
- 多种用户代理选择
- 随机请求延迟
- 反检测脚本注入
- 真实浏览器指纹模拟

## 技术栈

| 技术 | 版本 | 用途 |
|------|------|------|
| Python | 3.8+ | 主要编程语言 |
| Playwright | 1.48.0 | 浏览器自动化框架 |
| fake-useragent | 1.5.1 | 用户代理生成 |

## 项目结构

```
xiaohongshu-crawler/
├── config.py              # 配置文件
├── crawler_v2.py          # V2版爬虫（推荐）
├── crawler.py             # V1版爬虫（备用）
├── test_env.py            # 环境测试脚本
├── run.bat                # Windows启动脚本
├── run.sh                 # Linux/Mac启动脚本
├── requirements.txt       # Python依赖
├── .gitignore            # Git忽略文件
├── README.md             # 项目说明
├── QUICKSTART.md         # 快速开始指南
├── FILES.md              # 文件说明
├── example_output.json   # 输出示例
├── SUMMARY.md            # 本文件
├── output/               # 输出目录（自动创建）
│   ├── notes_*.json      # JSON格式数据
│   └── notes_*.csv       # CSV格式数据
└── cookies.json          # Cookies缓存（自动生成）
```

## 快速开始

### 1. 环境测试
```bash
python test_env.py
```

### 2. 首次运行
```bash
# Windows
run.bat

# Linux/Mac
./run.sh

# 手动运行
python crawler_v2.py
```

### 3. 扫码登录
- 等待浏览器打开
- 使用小红书APP扫码
- 登录成功后自动开始采集

## 配置说明

### 基本配置（config.py）

```python
# 搜索关键词
SEARCH_KEYWORD = "眼镜"

# 目标采集数量
TARGET_COUNT = 100

# 输出目录
OUTPUT_DIR = "./output"

# Cookie保存路径
COOKIE_FILE = "./cookies.json"

# 请求延迟（秒）
REQUEST_DELAY = 2

# 滚动延迟（秒）
SCROLL_DELAY = 1

# 无头模式
HEADLESS = False
```

## 工作流程

```
启动程序
  ↓
检查登录状态
  ↓
[未登录] 扫码登录 → 保存cookies
  ↓
搜索关键词
  ↓
滚动加载页面
  ↓
收集笔记链接
  ↓
逐个提取详情
  ↓
保存数据到文件
  ↓
更新cookies
  ↓
完成
```

## 输出格式

### JSON格式
```json
[
  {
    "title": "笔记标题",
    "content": "笔记正文内容",
    "url": "笔记链接",
    "crawl_time": "2026-01-05 20:00:00"
  }
]
```

### CSV格式
```csv
title,content,url,crawl_time
笔记标题,笔记正文内容,笔记链接,2026-01-05 20:00:00
```

## 版本对比

| 特性 | V1.0 | V2.0 |
|------|------|------|
| 基础搜索 | ✓ | ✓ |
| 扫码登录 | ✓ | ✓ |
| Cookies保存 | ✓ | ✓ |
| 数据导出 | ✓ | ✓ |
| 链接收集 | ✗ | ✓ |
| 链接去重 | ✗ | ✓ |
| 断点续传 | ✗ | ✓ |
| 错误恢复 | 基础 | 增强 |
| 页面选择器 | 单一 | 多种 |
| 推荐使用 | 否 | 是 |

## 注意事项

### 1. 反爬警告
- 小红书有反爬机制，请遵守以下规则：
  - 不要频繁请求（默认2秒间隔）
  - 不要一次采集过多数据（建议每次100篇以内）
  - 如果被限制，等待24小时后再试

### 2. 法律合规
- 仅用于个人学习和研究
- 不得用于商业用途
- 遵守小红书使用条款
- 尊重原作者版权

### 3. 登录维护
- cookies会自动保存
- cookies有效期为7-14天
- 失效后删除cookies.json重新登录
- 建议每周重新登录一次

### 4. 网络要求
- 需要稳定的网络连接
- 可以使用VPN，但不要太频繁切换
- 建议在非高峰时段运行

## 故障排除

### 常见问题

1. **环境测试失败**
   - 检查Python版本（需3.8+）
   - 重新安装依赖：pip install -r requirements.txt
   - 重新安装浏览器：playwright install chromium

2. **无法登录**
   - 删除cookies.json重新登录
   - 检查网络连接
   - 确保小红书APP是最新版本

3. **采集不到数据**
   - 检查是否已登录
   - 检查关键词是否正确
   - 尝试增加延迟时间

4. **程序卡住**
   - 可能是网络问题，等待或重试
   - 可能是被反爬，等待几小时再试
   - 尝试使用不同的网络

5. **浏览器闪退**
   - 检查系统资源
   - 尝试使用无头模式：HEADLESS = True
   - 重启程序

## 性能参数

### 采集速度
- 平均速度：约30-50篇/分钟
- 100篇需要：约2-3分钟
- 受网络和延迟参数影响

### 资源占用
- 内存：约200-300MB
- CPU：约10-20%
- 磁盘：每次采集约1-5MB

### 网络流量
- 每页约1-2MB
- 100篇约50-100MB

## 扩展建议

### 1. 功能扩展
- 支持采集图片和视频
- 支持采集评论数据
- 支持多关键词批量采集
- 支持定时自动采集
- 支持数据去重和更新

### 2. 性能优化
- 使用代理池
- 并发采集（注意反爬）
- 数据库存储
- 增量采集

### 3. 界面优化
- 添加GUI界面
- 实时显示采集进度
- 数据可视化
- 导出格式扩展（Excel、PDF等）

## 文档索引

| 文档 | 内容 | 适用场景 |
|------|------|----------|
| README.md | 项目完整说明 | 了解项目详情 |
| QUICKSTART.md | 快速开始指南 | 新手首次使用 |
| FILES.md | 文件说明 | 了解项目结构 |
| SUMMARY.md | 项目总结 | 快速了解全貌 |
| example_output.json | 输出示例 | 了解数据格式 |

## 更新日志

### V2.0 (2026-01-05)
- ✨ 新增链接收集功能
- ✨ 新增链接去重功能
- ✨ 改进错误处理机制
- ✨ 添加多种页面选择器
- 🐛 修复Windows编码问题
- 📝 完善文档

### V1.0 (2026-01-05)
- 🎉 初始版本发布
- ✨ 基础搜索和采集功能
- ✨ 扫码登录支持
- ✨ Cookies持久化
- ✨ JSON和CSV导出

## 贡献指南

如果你有改进建议或发现了bug，欢迎：

1. Fork本项目
2. 创建特性分支
3. 提交改动
4. 推送到分支
5. 创建Pull Request

## 许可证

本项目仅供学习研究使用。使用时请遵守相关法律法规和目标网站的使用条款。

## 免责声明

本工具仅用于技术研究和学习目的。开发者不对使用本工具造成的任何后果负责。使用本工具时，请遵守目标网站的使用条款和相关法律法规。

---

*项目创建日期: 2026-01-05*
*最后更新: 2026-01-05*
*维护者: AI Assistant*
