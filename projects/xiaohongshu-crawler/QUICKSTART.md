# 快速开始指南

## 3步开始使用

### 第1步：测试环境

```bash
python test_env.py
```

看到 ✅ 表示环境正常。

### 第2步：运行爬虫

**Windows用户：**
```bash
run.bat
```

**Linux/Mac用户：**
```bash
chmod +x run.sh
./run.sh
```

**手动运行：**
```bash
python crawler_v2.py
```

### 第3步：扫码登录

1. 等待浏览器打开
2. 使用小红书APP扫描屏幕上的二维码
3. 登录成功后自动开始采集

## 首次使用详细流程

### 1. 首次运行

当你第一次运行 `python crawler_v2.py` 时：

```
============================================================
  小红书爬虫 V2.0 - 眼镜相关笔记采集
============================================================
✓ 加载了 0 个cookies
⚠ 未登录
未检测到登录状态，需要登录

=== 开始登录流程 ===
📱 请使用小红书APP扫描二维码登录...
⏳ 等待登录（最多等待2分钟）...
```

此时会打开浏览器窗口，显示登录二维码。

### 2. 扫码登录

1. 打开手机上的小红书APP
2. 点击 "我的" → "设置" → "扫码登录"
3. 扫描屏幕上的二维码
4. 在手机上确认登录

### 3. 开始采集

登录成功后，程序会自动：

```
✓ 登录成功！
✓ Cookies已保存到 cookies.json
✓ 已检测到登录状态

🔍 正在搜索: 眼镜
📜 开始滚动收集笔记链接...
✓ 从列表页提取到 20 个新链接
📊 当前已收集 20 个链接
...

📝 开始提取 100 篇笔记详情...
✓ 成功提取: 2025年最火的5款眼镜推荐！...
📌 [1/100] 2025年最火的5款眼镜推荐！...
✓ 成功提取: 圆脸适合什么眼镜？...
📌 [2/100] 圆脸适合什么眼镜？...
...
```

### 4. 完成采集

采集完成后：

```
✓ 成功保存 100 篇笔记到 output/notes_20260105_200000.json
✓ 成功保存 100 篇笔记到 output/notes_20260105_200000.csv
✓ Cookies已保存到 cookies.json

============================================================
✅ 爬取完成！共采集 100 篇笔记
============================================================
```

## 常见问题

### Q1: 浏览器打开后什么都没有？

**A:** 可能是网络问题或页面加载慢，请：
- 检查网络连接
- 等待10-20秒
- 如果还是不行，尝试关闭VPN

### Q2: 扫码后提示登录失败？

**A:** 可能的原因：
- 二维码已过期（等待太久），重新运行程序
- 小红书APP版本过旧，请更新APP
- 网络不稳定，重试即可

### Q3: 采集速度很慢？

**A:** 这是正常的，为了不被反爬，程序设置了延迟：
- 每次请求间隔2秒
- 滚动间隔1秒
- 这是保护措施，请耐心等待

### Q4: 采集到一半停止了？

**A:** 可能的原因：
- 网络连接中断
- 小红书限制了访问（等几小时再试）
- cookies过期（删除cookies.json重新登录）

### Q5: 如何更换搜索关键词？

**A:** 编辑 `config.py` 文件：

```python
# 将
SEARCH_KEYWORD = "眼镜"

# 改为
SEARCH_KEYWORD = "墨镜"
# 或
SEARCH_KEYWORD = "近视眼镜"
```

### Q6: 数据保存在哪里？

**A:** 所有数据保存在 `output` 目录：
- `notes_YYYYMMDD_HHMMSS.json` - JSON格式
- `notes_YYYYMMDD_HHMMSS.csv` - CSV格式

### Q7: 如何查看采集的数据？

**A:** 推荐方式：
1. 打开CSV文件，使用Excel打开
2. 或者用文本编辑器打开JSON文件
3. 示例数据见 `example_output.json`

### Q8: cookies失效怎么办？

**A:** 重新登录：

```bash
# Windows
del cookies.json
python crawler_v2.py

# Linux/Mac
rm cookies.json
python crawler_v2.py
```

## 高级使用

### 调整采集数量

编辑 `config.py`：

```python
TARGET_COUNT = 50  # 改为50篇
```

### 调整采集速度

编辑 `config.py`：

```python
REQUEST_DELAY = 3  # 请求延迟改为3秒
SCROLL_DELAY = 2   # 滚动延迟改为2秒
```

### 使用无头模式（后台运行）

编辑 `config.py`：

```python
HEADLESS = True  # 后台运行，不显示浏览器窗口
```

注意：无头模式下无法扫码登录，建议先用HEADLESS=False登录并保存cookies，再改为True。

### 定时采集

可以使用Windows任务计划程序或Linux的cron定时运行：

```bash
# Linux/Mac crontab -e
# 每天凌晨2点采集
0 2 * * * cd /path/to/xiaohongshu-crawler && python crawler_v2.py >> log.txt 2>&1
```

## 注意事项

1. **法律合规**：
   - 仅用于个人学习和研究
   - 不得用于商业用途
   - 遵守小红书使用条款

2. **反爬警告**：
   - 不要频繁运行
   - 不要一次采集过多数据（建议每次100篇以内）
   - 如果被限制，等待24小时后再试

3. **数据使用**：
   - 采集的数据仅供个人学习使用
   - 不要公开发布采集的数据
   - 尊重原作者的版权

---

*祝你使用愉快！如有问题，请查看 README.md 的故障排除部分*
