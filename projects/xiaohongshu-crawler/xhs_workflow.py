"""
å°çº¢ä¹¦å®Œæ•´å·¥ä½œæµ - é‡‡é›† â†’ åˆ†æ â†’ ç”Ÿæˆ
"""
import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime

# è®¾ç½®UTF-8ç¼–ç è¾“å‡º
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')


class XHSWorkflow:
    """å°çº¢ä¹¦å·¥ä½œæµç®¡ç†å™¨"""

    def __init__(self, keyword, count=5):
        self.keyword = keyword
        self.count = count
        self.output_dir = Path("output")
        self.output_dir.mkdir(exist_ok=True)

    def find_latest_notes(self):
        """æŸ¥æ‰¾æœ€æ–°é‡‡é›†çš„ç¬”è®°"""
        pattern = f"notes_{self.keyword}_*incremental.json"
        files = list(self.output_dir.glob(pattern))

        if not files:
            print(f"âŒ æœªæ‰¾åˆ°å…³é”®è¯'{self.keyword}'çš„é‡‡é›†ç¬”è®°")
            print(f"   è¯·å…ˆè¿è¡Œçˆ¬è™«é‡‡é›†ç¬”è®°ï¼špython crawler_v4.py")
            return None

        # è·å–æœ€æ–°æ–‡ä»¶
        latest_file = max(files, key=lambda f: f.stat().st_mtime)
        print(f"âœ… æ‰¾åˆ°æœ€æ–°é‡‡é›†ç¬”è®°ï¼š{latest_file.name}")

        return latest_file

    def analyze_notes(self, notes_file):
        """åˆ†æé‡‡é›†çš„ç¬”è®°"""
        print("\n" + "="*60)
        print("ã€ç¬¬ä¸€æ­¥ã€‘åˆ†æé‡‡é›†ç¬”è®°ç‰¹ç‚¹")
        print("="*60)

        with open(notes_file, 'r', encoding='utf-8') as f:
            notes = json.load(f)

        analysis = {
            "note_count": len(notes),
            "title_length": [],
            "content_length": [],
            "use_emoji": 0,
            "use_tags": 0,
            "types": {"æ•™ç¨‹æ”»ç•¥": 0, "ç©¿æ­åˆ†äº«": 0, "æ—¥å¸¸è®°å½•": 0, "äº§å“åˆé›†": 0}
        }

        for note in notes:
            title = note['title']
            content = note['content']

            analysis["title_length"].append(len(title))
            analysis["content_length"].append(len(content))

            # æ£€æµ‹emoji
            if any(ord(c) > 127 for c in title):
                analysis["use_emoji"] += 1

            # æ£€æµ‹æ ‡ç­¾
            if '#' in content:
                analysis["use_tags"] += 1

            # åˆ†ç±»
            text = title + content
            if any(word in text for word in ['æ”»ç•¥', 'æ•™ç¨‹', 'å…¬å¼', 'å¦‚ä½•']):
                analysis["types"]["æ•™ç¨‹æ”»ç•¥"] += 1
            elif any(word in text for word in ['æ­é…', 'ç©¿æ­', 'éŸ©ç³»']):
                analysis["types"]["ç©¿æ­åˆ†äº«"] += 1
            elif any(word in text for word in ['æ—¥å¸¸', 'è®°å½•', 'é˜³å…‰', 'å¹¸ç¦']):
                analysis["types"]["æ—¥å¸¸è®°å½•"] += 1
            else:
                analysis["types"]["äº§å“åˆé›†"] += 1

        # æ˜¾ç¤ºåˆ†æç»“æœ
        print(f"\nğŸ“Š é‡‡é›†ç¬”è®°æ•°é‡ï¼š{analysis['note_count']}ç¯‡")
        print(f"ğŸ“ å¹³å‡æ ‡é¢˜é•¿åº¦ï¼š{sum(analysis['title_length'])/len(analysis['title_length']):.1f}å­—ç¬¦")
        print(f"ğŸ“ å¹³å‡å†…å®¹é•¿åº¦ï¼š{sum(analysis['content_length'])/len(analysis['content_length']):.0f}å­—ç¬¦")
        print(f"ğŸ˜Š ä½¿ç”¨emojiæ¯”ä¾‹ï¼š{analysis['use_emoji']}/{analysis['note_count']} ({analysis['use_emoji']/analysis['note_count']*100:.0f}%)")
        print(f"ğŸ·ï¸  ä½¿ç”¨æ ‡ç­¾æ¯”ä¾‹ï¼š{analysis['use_tags']}/{analysis['note_count']} ({analysis['use_tags']/analysis['note_count']*100:.0f}%)")

        print(f"\nğŸ“š ç¬”è®°ç±»å‹åˆ†å¸ƒï¼š")
        for note_type, count in analysis["types"].items():
            if count > 0:
                print(f"   - {note_type}ï¼š{count}ç¯‡")

        return analysis

    def generate_notes(self, analysis):
        """åŸºäºåˆ†æç»“æœç”Ÿæˆæ–°ç¬”è®°"""
        print("\n" + "="*60)
        print("ã€ç¬¬äºŒæ­¥ã€‘ç”Ÿæˆå°çº¢ä¹¦ç¬”è®°")
        print("="*60)

        # å¯¼å…¥ç”Ÿæˆå™¨
        from generate_notes import XHSNoteGenerator

        generator = XHSNoteGenerator(self.keyword)
        notes = generator.generate(count=self.count)

        # æ˜¾ç¤ºç”Ÿæˆç»“æœ
        print(f"\nâœ¨ æˆåŠŸç”Ÿæˆ{len(notes)}ç¯‡ç¬”è®°ï¼š\n")

        type_count = {}
        for note in notes:
            note_type = note['type']
            type_count[note_type] = type_count.get(note_type, 0) + 1

            print(f"ã€{note_type}ã€‘{note['title']}")
            print(f"   é•¿åº¦ï¼š{len(note['content'])}å­—ç¬¦ | æ ‡ç­¾ï¼š{note['content'].count('#')}ä¸ª\n")

        print(f"ğŸ“Š ç±»å‹åˆ†å¸ƒï¼š{type_count}")

        # ä¿å­˜æ–‡ä»¶
        json_file, txt_file = generator.save(notes)

        return notes, json_file, txt_file

    def test_quality(self, notes, analysis):
        """æµ‹è¯•ç”Ÿæˆç¬”è®°çš„è´¨é‡"""
        print("\n" + "="*60)
        print("ã€ç¬¬ä¸‰æ­¥ã€‘è´¨é‡æ£€æµ‹")
        print("="*60)

        passed = 0
        failed = 0

        for i, note in enumerate(notes, 1):
            issues = []

            # æ£€æŸ¥æ ‡é¢˜é•¿åº¦
            title_len = len(note['title'])
            avg_title_len = sum(analysis['title_length']) / len(analysis['title_length'])
            if not (avg_title_len * 0.5 <= title_len <= avg_title_len * 1.5):
                issues.append(f"æ ‡é¢˜é•¿åº¦å¼‚å¸¸ï¼ˆ{title_len}å­—ç¬¦ï¼‰")

            # æ£€æŸ¥å†…å®¹é•¿åº¦
            content_len = len(note['content'])
            avg_content_len = sum(analysis['content_length']) / len(analysis['content_length'])
            if not (avg_content_len * 0.3 <= content_len <= avg_content_len * 2):
                issues.append(f"å†…å®¹é•¿åº¦å¼‚å¸¸ï¼ˆ{content_len}å­—ç¬¦ï¼‰")

            # æ£€æŸ¥æ ‡ç­¾
            if note['content'].count('#') < 3:
                issues.append("æ ‡ç­¾æ•°é‡ä¸è¶³")

            # æ£€æŸ¥emoji
            if not any(ord(c) > 127 for c in note['title']):
                issues.append("æ ‡é¢˜ç¼ºå°‘emoji")

            if issues:
                failed += 1
                status = "âŒ æœªé€šè¿‡"
            else:
                passed += 1
                status = "âœ… é€šè¿‡"

            print(f"\nç¬”è®°{i}ï¼š{note['title'][:20]}...")
            print(f"  {status}")
            if issues:
                for issue in issues:
                    print(f"    - {issue}")

        # æ€»ç»“
        print("\n" + "-"*60)
        print(f"è´¨é‡æ£€æµ‹å®Œæˆï¼š")
        print(f"  âœ… é€šè¿‡ï¼š{passed}/{len(notes)} ({passed/len(notes)*100:.0f}%)")
        print(f"  âŒ æœªé€šè¿‡ï¼š{failed}/{len(notes)} ({failed/len(notes)*100:.0f}%)")

        if passed == len(notes):
            print("\nğŸ‰ æ‰€æœ‰ç¬”è®°è´¨é‡æ£€æµ‹é€šè¿‡ï¼")
        else:
            print("\nâš ï¸ éƒ¨åˆ†ç¬”è®°éœ€è¦ä¼˜åŒ–")

    def run(self):
        """è¿è¡Œå®Œæ•´å·¥ä½œæµ"""
        print("\n" + "="*60)
        print("å°çº¢ä¹¦ç¬”è®°ç”Ÿæˆå·¥ä½œæµ")
        print("="*60)
        print(f"å…³é”®è¯ï¼š{self.keyword}")
        print(f"ç”Ÿæˆæ•°é‡ï¼š{self.count}ç¯‡")
        print(f"å¼€å§‹æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # 1. æŸ¥æ‰¾é‡‡é›†ç¬”è®°
        notes_file = self.find_latest_notes()
        if not notes_file:
            return False

        # 2. åˆ†æé‡‡é›†ç¬”è®°
        analysis = self.analyze_notes(notes_file)

        # 3. ç”Ÿæˆæ–°ç¬”è®°
        notes, json_file, txt_file = self.generate_notes(analysis)

        # 4. è´¨é‡æ£€æµ‹
        self.test_quality(notes, analysis)

        # å®Œæˆ
        print("\n" + "="*60)
        print("âœ… å·¥ä½œæµå®Œæˆï¼")
        print("="*60)
        print(f"\nç”Ÿæˆçš„ç¬”è®°å·²ä¿å­˜è‡³ï¼š")
        print(f"  ğŸ“„ JSONæ ¼å¼ï¼š{json_file}")
        print(f"  ğŸ“ æ–‡æœ¬æ ¼å¼ï¼š{txt_file}")

        return True


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description="å°çº¢ä¹¦ç¬”è®°ç”Ÿæˆå·¥ä½œæµ")
    parser.add_argument("keyword", help="å…³é”®è¯ï¼ˆå¦‚ï¼šå¥³ç”Ÿçœ¼é•œæ¨èï¼‰")
    parser.add_argument("-n", "--count", type=int, default=5, help="ç”Ÿæˆæ•°é‡ï¼ˆé»˜è®¤5ç¯‡ï¼‰")

    args = parser.parse_args()

    # åˆ›å»ºå·¥ä½œæµ
    workflow = XHSWorkflow(args.keyword, args.count)

    # è¿è¡Œ
    success = workflow.run()

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
